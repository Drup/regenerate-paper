\section{Related Work}
\label{sec:related-work}

\paragraph{Regular Language Generation}

\citet{DBLP:journals/actaC/Makinen97} describes a method to enumerate
the words of a regular language $L$, given by a deterministic finite
automaton, in length lexicographic ordering. To generate words up to
length $n$, this method precomputes in time $O(n)$, for each $i\le n$,
the lexicographically minimal and maximal word of length $i$ in $L$.
%
Enumeration starts with the minimal word of
length $n$ and repeatedly computes the lexicographically next word in $L$
until it reaches the maximal word of length $n$. Each step requires time $O(n)$. 

% The same approach can be used for enumerating the language of certain
% (prefix-free, length complete) context-free grammars, too.

In comparison, M{\"{a}}kinen requires a deterministic
finite automaton, which can be obtained from a regular expression in
worst-case exponential time. Complementation is not mentioned, but it
could be handled. M{\"{a}}kinen would give rise to a
productive definition by cross sections because the computation of minimal
and maximal words could be done incrementally, but which is not mentioned
in the paper.

% Implementation of McIlroy
% % https://github.com/tadeboro/reglang

\citet{DBLP:journals/jfp/McIlroy04} implements the enumeration of all
strings of a regular language in Haskell. He develops two approaches,
one based on interpreting regular expressions inspired by
\citet{misra11:_enumer_strin_regul_expres} and discussed in
Section~\ref{sec:naive-approach}, the other (unrelated to ours) using
a shallow embedding of nondeterministic finite automata.

% The implementation of union is identical to ours, but intersection and
% difference operations are not considered and hence complementation is
% not considered, either. The implementation of concatenation is the
% generic multiplication operation for sequences / power series
% \cite{DBLP:journals/jfp/McIlroy99} instantiated for the semiring
% of union and concatenation of languages. Unlike our implementation, the generic 
% implementation does not take advantage of the fact that many
% intermediate results can be generated in the correct ordering and hence
% requires many more union operations (one for each output string versus
% one for each length between $0$ and $n$ where $n$ is the length of
% the output string).  Moreover, the generation method is reported to
% be very inefficient and thus not suitable for generating test inputs
% at a large scale.

\citet{DBLP:journals/tcs/AckermanS09} improve M{\"{a}}kinen's
algorithm by working on a nondeterministic finite automaton
and by proposing faster algorithms to compute minimal words of a given
length and to proceed to the next word of same length. An empirical
study compares a number of variations of the enumeration algorithm.
%
% Their enumeration algorithm iteratively invokes a cross-section
% enumeration, where the $n^{\text{th}}$ cross-section of a language $L$ is
% $L \cap \Sigma^n$, that is, a segment in our terminology.
%
\citet{DBLP:conf/cocoon/AckermanM09} present three further
improvements on their enumeration algorithms with better asymptotic
complexity. The improved algorithms perform better in practice, too.

Ackerman's approach and its subsequent improvement does not incur an
exponential blowup when converting from a regular expression. As it is based on
nondeterministic finite automata, complementation cannot readily be
supported. Moreover, the approach is not compositional.

% They critize his complexity analysis, which assumes unit
% cost for comparison and concatenation operations on words and does not
% account for the size $s$ of the automaton, and obtain $O (s^2n^2)$ for
% the computation of minimal words.

% As one example of a line of unrelated work with deceivingly similar
% titles, \citet{DBLP:conf/wia/LeeS04} discuss enumerating regular
% expressions and their languages. The goal of this work is aims to find
% bounds on the \textbf{number of languages} that can be represented
% with regular expressions and automata of a certain size.



\paragraph{Language Generation}
Some authors discuss the generation of test sentences from grammars for
exercising compilers
(e.g., \cite{DBLP:conf/cisse/ParachaF08,DBLP:conf/compsac/ZhengW09}
for some recent work). This
line of work goes back to Purdom's sentence generator for testing
parsers \cite{purdom72:_senten_gener_testin_parser}, which creates
sentences from a context-free grammar using each production at least
once.

Compared to our generator, the previous work starts from context-free
languages and aims at testing the apparatus behind the parser,
rather than the parser itself. Hence, it focuses on generating
positive examples, whereas we are also interested in counterexamples.

Grammar Testing~\cite{DBLP:conf/fase/Lammel01} aims to
identify and correct errors in a grammar by exercising it on example
sentences. The purpose is to recover ``lost'' grammars of programming
languages effectively.
Other work \cite{DBLP:conf/compsac/LiJLG04} also targets
testing the grammar, rather than the parser.

\paragraph{Test Data Generation}

Since the introduction of the library
QuickCheck~\cite{DBLP:conf/icfp/ClaessenH00}, property testing and
test-data generation has been used successfully in a wide variety of
contexts.  In property testing, input data for the function to test is
described via a set of combinators while the actual generation is
driven by a pseudo-random number generator. One difficulty of this
approach is to find a distribution of inputs that will
generate challenging test cases. This problem already arises with
recursive data types, but it is even more pronounced when generating
test inputs for regular expressions because, as explained in
\cref{sec:motivation}, many languages have a density of zero, which
means that a randomly generated word almost never belongs to the
langer.  Generating random \emph{regular expressions} is much
easier. We can thus combine property testing to generate regular
expressions and then apply our language generator to generate targeted
positive and negative input for these randomly generated regular
expressions.

\citet{DBLP:journals/jfp/NewFFM17} enumerate elements of various data
structures. Their approach is complementary to test-data
generators. It exploits bijections between natural numbers and the
data domain and develops a quality criterion for data generators based
on fairness.
% It would be interesting to investigate the
% connection between their enumeration strategies and a direct
% representation of formal power series.

Crowbar~\cite{crowbar} is a library that combines property testing
with fuzzing.  In QuickCheck, the generation is driven by a random
number generator. Crowbar replaces this generator by
\texttt{afl-fuzz}~\cite{afl}. Afl is a fuzzer that relies on runtime
instrumentation to provide good code coverage, thus eliminating the
need to specify the distribution of random generators.  This approach,
however, is not sufficient to generate both regular expressions and
inputs, as we would still require an oracle. Our language generator
could allow to easily fuzz regular expression parsers.


% \subsubsection*{Testing Program Generators}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
